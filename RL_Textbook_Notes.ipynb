{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10250398-4f88-4955-8b8e-3774ec81122a",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ededc308-d24a-4cbe-bf0e-85244f64c789",
   "metadata": {},
   "source": [
    "***k-armed Bandit Problem***: Analogy of having several attempts at a slot machine with k levers. Each of the levers has an expected or mean reward value we receive if we decide to pull it (select lever 5 as our action at timestep 3 for example). The action selected at timestep t is denoted as A_t, the reward from this action is denoted as R_t, our arbitary action select as a, and the expected reward formula is thus: q*(a) = E[R_t | A_t = a]\n",
    "\n",
    "***Exploration vs Exploitation***: This is a tradeoff in RL, exploration is trying out new levers (with a potentially lower expected reward) with the hopes that we receive a much higher reward. Exploitation is using our current knowledge of expected rewards and selecting the action with the highest expected reward. We use different models to switch between these two efforts when we have our agent work through our map trying to collect rewards.\n",
    "\n",
    "***Sample-average***: Sampling method for creating an expected value estimate on an action. This method is simply the average reward value of the action up to this point. Meaning we sum all the rewards we got whenever we took this specific action then divided it by the amount of times we took this specific action. \n",
    "\n",
    "***Greedy action selection***: A_t = argmax_a Q_t(a) (select the action a that will maximize the value that Q_t (expected reward) will return.\n",
    "\n",
    "***epsilon-greedy methods***: methods that have a preset variable value epsilon that represents a probability that our agent will select an action randomly (all actions with equal probability) instead of the action that has the highest expected reward. Ex: eps=0.5 for the case of two actions at timestep 1: 50% chance of selecting action with highest expected prob and 50% chance of selecting randomly between action 1 and 2 (thus 0.50 + 0.25 = 0.75 chance we select the greedy option (excercise 2.1)).\n",
    "\n",
    "\n",
    "Advantage of eps-greedy over greedy depends on the task. For example, if the rewards have higher variance then it takes more exploration to find the true best rewards. If there is no variance then no exploration is needed and we can just do greedy. Can also run into ***nonstationary tasks*** where the true reward value of an action changes over time.\n",
    "\n",
    "***deterministic case***: models based on predefined logic and rules, no randomness is involved.\n",
    "\n",
    "Memory efficient way to calculate the sample-average reward (without storing all of the previous reward values). The update formula is: Q_n+1 = Q_n + (1/n)[R_n - Q_n] ==> NewEst = OldEst + StepSize[Target - OldEst]\n",
    "\n",
    "\n",
    "In nonstationary tasks we want to add more weighting to the most recent rewards observed (as tyring to get closest representation of the expected reward for the action currently (assuming it has changed)). We can change our stepsize function from the sample average function alpha(a) = (1/n) to a constant value alpha: alpha(a) = alpha. The sample average stepsize is gauranteed to converge to the true mean expected value while the constant value isnt. This is good becuase we dont want convergence on an expected value that is constantly changing.\n",
    "\n",
    "\n",
    "***Optimistic initial values***: setting the initial sample reward of each action much higher to what you think it will be. This encourages the agent to explore more at the start which can help for stationary problems. (Doesnt for non stationary as eventually expected reward will change and the inital reward values wont matter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5050e8af-1302-4f82-9be7-31c39f37c722",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77b75a4-5cba-41b6-aaa3-4f1a7f20096b",
   "metadata": {},
   "source": [
    "***Markov decision process***: model only depends on the previous state. In terms of RL, the model depends only on the previous state and action taken to determine its new state and current reward:\n",
    "\n",
    "    p(s', r | s, a) = Pr(S_t = s', R_t = r | S_t-1 = s, A_t-1 = a).\n",
    "    Creating the sequence: S_0,A_0,R_1,S_1,A_1,R_2,S_2,A_2,R_3,...\n",
    "\n",
    "Each probability function for state and reward returns a probability from a probability distribution. For each possible state (s) and action (a) we have a distribution of the probability for each of the next possible new_state and reward pairs occuring, given the previous state and action taken:  \n",
    "\n",
    "    SUM:s'_in_S SUM:r_in_R p(s', r | s, a) = 1, For_All s in S, a in A(s)\n",
    "\n",
    "***Markov property***: the likelihood of changing to a specific state is dependently only on the current state (and the current time elapsed), and not at all on any of the previous states. \"The state must include information about all aspects of the past agent–environment interaction that make a difference for the future\".\n",
    "\n",
    "***State transition probabilities***: The probability of transitioning from the current state to some specific next state. This is done by summing all of the reward probabilities for moving from the current state to this specific next state. In other words, we dont care about which reward we're gonna get, only what the probability is that we move to this next state:\n",
    "    \n",
    "    p(s' | s, a) = SUM:r_in_R p(s', r | s, a)\n",
    "\n",
    "***Expected reward for state-action pairs***: The expected reward we get for taking a specific action (a) at our current state (s):\n",
    "\n",
    "    r(s, a) = E[R_t | S_t-1=s, A_t-1 = a] = SUM:r_in_R *r* SUM:s'_in_S p(s', r | s, a)\n",
    "\n",
    "(Sum of each of the rewards (r) multiplied by the sum of each of the possible states we can get to (all s') if r was our reward)\n",
    "\n",
    "\n",
    "***Expected reward for state–action–next-state***: The expected reward we can expect to get if we take action (a) at our current state (s) and arrive at the new state (s'):\n",
    "\n",
    "    r(s, a, s') = E[R_t | S_t-1=s, A_t-1=a, S_t=s'] = SUM:r_in_R *r* p(s', r | s, a) / p(s' | s, a)\n",
    "\n",
    "    (Sum of each of the rewards (r) multiplied by the probability of getting our reward (r) at the new state (s') divided by the probability of getting our new state (s') from our current state (s) and action taken (a)).\n",
    "\n",
    "(The probability of getting a specific reward (r) at our specific next state (s') can be low, but if we factor in the probability of getting to this new state in general that will make our probability of receiving that reward realistic. In other words, what are the chances of getting our reward (r) in a specific scenerio given that this specific scenerio actually happens).\n",
    "\n",
    "***Agent–environment boundary***: Anything that is outside of the agents control and \"cannot be changed arbitrarely by the agent\" is considered to be out of the agents control and part of the enviornment. Thus the boundary represents the limit of the agents absolute control. For example, a robot (which is our agent) has mechanical arms and sensory devices, these are considered part of its enviornment and not part of the agent as they are reactive to the current enviornment and actions taken so far.\n",
    "\n",
    "***Reward hypothesis***: The agents goal is to maximize the total amount of reward it receives. Therefore not maximize immediate reward but average reward in the long run. \"That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward)\".\n",
    "\n",
    "***Episodes***: \"when the agent–environment interaction breaks naturally into subsequences, ... such as plays of a game, trips through a maze, or any sort of repeated interaction\". \n",
    "\n",
    "Each episode ends in a special state called the ***terminal state***.\n",
    "\n",
    "\"the next episode begins independently of how the previous one ended. Thus the episodes can all be considered to end in the same terminal state, with di↵erent rewards for the di↵erent outcomes. Tasks with episodes of this kind are called ***episodic tasks***\"\n",
    "\n",
    "***Continuing tasks***: \"in many cases the agent–environment interaction does not break naturally into identifiable episodes, but goes on continually without limit\". Ex: Robot with a long life span.\n",
    "\n",
    "Issue with continuing tasks is that our final time step is T=infinity, thus it is very easy for our agent to get a average reward of infinity and perform suboptimally as eventually it will reach a reward of infinity no matter how many sub optimal actions it takes. To solve this we use discounting in our rewards calculation.\n",
    "\n",
    "***Discounting***: Adding a hyperparameter that is a function of our current timestep. This hyperparameter multiplies our current reward by some preset constant value taken to the power of our current timestep:\n",
    "\n",
    "    G_t = R_t+1 + \u0000\u0000gamma*R_t+2 + gamma^2*R_t+3 + ... = inf_SUM_k=0 gamma^k * R_t+k+1\n",
    "\n",
    "gamma is in the range [0, 1] and refered to as the discount rate. \"Gamma determins the present value of future rewards, a reward received\n",
    "k time steps in the future is worth only \u0000gamma^(k\u0000-1) times what it would be worth if it were received immediately\". The longer you take (the larger your timestep gets) the less your reward value will be (creates sense of urgency for the agent).\n",
    "\n",
    "if gamma = 0 then we use none of the future reward calculations in our discounted reward so the model only maximizes the immediate reward and becomes greedy, versus as gamma gets closer to 1 it adds more weight to the future reward terms and the model becomes more faresighted.\n",
    "\n",
    "    G_t = R_t+1 + gamma*R_t+2 + gamma^2*R_t+3 + gamma^3*R_t+4 + ...\n",
    "    = R_t+1 + gamma(R_t+2 + gamma*R_t+3 + ...)\n",
    "    = R_t+1 + gamma*G_t+1\n",
    "\n",
    "\\*In these continuous tasks we have estimates for future reward to be received given a specific action a. These future rewards are based on the actions we expect the model to take and expected reward to receive (based on those future actions). And these future actions are the ones that will be available to us only if we take this certain action a. Thus this is a forward \"future\" thinking model and the gamma value is our hyperparameter for how much trust it has into these expected future values.*\n",
    "\n",
    "\n",
    "***Policy***: An agents strategy of behaviour (which actions it chooses) at a given time. \"Formally, a policy is a mapping from states to probabilities of selecting each possible action\". Example: in maze problem, dumb agents policyt is to wander around, while smart agents policy is to plan path in head first then go straight to the end goal. \n",
    "\n",
    "if agent is following policy pi() at time t, then pi(a|s) is probability that A_t = a, if S_t = s. \n",
    "The pi() function gives one probability but represents given a state s, ouput a probability distribution of actions the agent will take.\n",
    "\n",
    "Excercise 3.11: If the current state is St, and actions are selected according to stochastic policy ⇡, then what is the expectation of Rt+1 in terms of pi() and the four-argument function p (3.2)? -> pi(a|s) * r(s, a) forAll actions a in Action set of state s. the probability of taking the action multiplied by the expected reward value of taking that action at the current state s. (unsure on this one) not using the four argument function (3.2): p(s', r | s, a). \n",
    "\n",
    "***Deterministic policy***: Each state maps to a specific action with certanty. Meaning when we end up in state s1 we know that the policy will take action a1 with 100% probability.\n",
    "\n",
    "***Stochastic policy***: There is a distribution among the possible set of actions that can be taken from a certain state. Meaning there are different probabilities that the policy selects to take different actions at a state s. (Each of the probabilities from all the possible actions sum to 1).\n",
    "\n",
    "***State-value function for policy pi()***: The expected reward (from a continuous enviornment) we can expect to get if we start at state s and follow our specific policy pi().\n",
    "\n",
    "    v_pi(s) = E_pi[G_t| S_t = s] = E_pi[inf_SUM_k=0 gamma^k * R_t+k+1 | S_t=s ], forAll s in S\n",
    "\n",
    "***Action-value function for policy pi()***: The expected reward (from a cont. env.) we can expect to get if we start at state s, take action a, then follow our specific policy pi() after (after action a is taken at state s follow policy - what is the expected reward?).\n",
    "\n",
    "    q_pi(s, a) = E_pi[G_t| S_t = s, A_t = a] = E_pi[inf_SUM_k=0 gamma^k * R_t+k+1 | S_t=s, A_t = a ]\n",
    "\n",
    "***Monte Carlo method***: Calculating an average value from many samples taken from a random variable converges to the real expected value as the number of samples converges to infinity.\n",
    "\n",
    "***Optimal policy***: The optimal policy is the policy that gives us the highest summed reward over the long run. More formally as a form of ranking, a policy pi_1 is said to be better than or equal to a policy pi_2, if the expected return value of pi_1 is greater than or equal to the expected return of pi_2 for each state. Or: pi_1 >= pi_2 iff v_pi_1(s) >= v_pi_2(s) forAll s in S. Due to the greater than or equal to, there may be more than one optimal policy to which they are denoted as pi_*. \n",
    "\n",
    "Each share the same: \n",
    "\n",
    "***optimal state-value function***: \n",
    "    \n",
    "    v_*(s) = max_pi v_pi(s) forAll s in S. \n",
    "\n",
    "***optimal action-value function***: \n",
    "\n",
    "    q_*(s, a) = max_pi q_pi(s, a) forAll s in S, a in A = E[R_t+1 + gamma\\*v_\\*(S_t=t+1) | S_t = a, A_t = a]\n",
    "\n",
    "***Bellman optimality equation***: The value of a state value function following an optimal policy is equal to the expected return for the best action we can take from that state s.\n",
    "\n",
    "    v_*(s) = max_a_in_A(s) q_pi_*(s, a)\n",
    "    = max_a E_pi_*[G_t | S_t = s, A_t = a]\n",
    "    = max_a SUM_s'_r p(s', r| s, a)[r + gamma*V_*(s')]\n",
    "    select the action (a) that will return the max value of the probability of getting each reward at each possible next state multiplied by the reward we receive (for action a) summed with the gamma multiplied optimal state value function on that next possible state instance.\n",
    "\n",
    "    \n",
    "\n",
    "Meaning, starting at a state s, if we are following an optimal policy we will select the best possible action (in other words the action that has the highest expected reward)\n",
    "\n",
    "The same Bellman optimality logic applies for the action-value function: \n",
    "\n",
    "    q_*(s, a) = E[R_t+1 + gamma*Max_a' q_*(S_t+1, a') | S_t = s, A_t = a]\n",
    "    = SUM_s'_r p(s', r | s, a)[r + gamma* max_a' q_*(s', a')]\n",
    "\n",
    "\n",
    "The beauty of having an optimal state-value function is that we can create an agent that selects the action in a greedy way by only calculating the reward for this current state on which action to take (only looks in the present and not in the future). But the model is not short sighted because the optimal reward values from the state-value function are calculated with respect to the expected future rewards so the model is actually looking into the future.\n",
    "\n",
    "Often times we dont have either 1. an entire knowledge of the enviornment and its states 2. enough memory and computation power to calculate the optimal expected value for each state to get an optimal function to follow.\n",
    "\n",
    "***Tabular case***: An enviornment whos state set is small enough so that we can create an entry for a expected sampled reward value for each state (or each state-action pair) as a table or array in a computers memory.\n",
    "\n",
    "\"In many cases of practical interest, however, there are far more states than could possibly be entries in a table. In these cases the functions must be approximated, using some sort of more compact parameterized function representation.\"\n",
    "\n",
    "There can exist states that have an extremely low probability of being seen or stumbled upon by an agent that taking a suboptimal action for these states likely wont affect the overall expected total reward. Ex: Taking suboptimal: (0.000002)*(2) vs (0.000002)*4. (The prob of this state occuring (after a certain action) multiplied by the reward we get from it. Even though we took reward 2 instead of 4, with the probability being so low, our gain in expected reward doesnt change very much if at all).\n",
    "\n",
    "\"The online nature of reinforcement learning makes it possible to approximate optimal policies in ways that put more effort into learning to make good decisions for frequently encountered states, at the expense of less effort for infrequently encountered states. This is one key property that distinguishes reinforcement learning from other approaches to approximately solving MDPs.\"\n",
    "\n",
    "\"In reinforcement learning we are very much concerned with cases in which optimal solutions cannot be found but must be approximated in some way.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b79154-a48d-444a-baa0-ad95041154ec",
   "metadata": {},
   "source": [
    "# Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccfd5ab-caca-462c-b20f-b4c65694510c",
   "metadata": {},
   "source": [
    "***Policy evaluation (prediction problem)***: The process we go through or the problem of trying to figure out the expected reward from a given state s following policy pi. In other words figuring out the value of the state-value function at state s: \n",
    "    \n",
    "    v_pi(s) = E_pi[G_t | S_t = s]\n",
    "\n",
    "***Iterative policy evaluation***: An iterative approach to policy evaluation. For example, we know the reward we will get if we step into our goal state. But we then need to figure out the expected reward value of reaching all the possible states before the goal state and use that to figure out the expected reward value of all the states that can be used to reach those previous before reward states. This results in a system of linear equations that can be solved (only if we know the entire enviornment (aka know all possible states)) but this method uses an approximative iterative alternative approach.\n",
    "\n",
    "    v_k+1(s) = E_pi[R_t + gamma*v_k(S_t+1) | S_t = s ] \n",
    "    = SUM_a pi(a|s) SUM_s' SUM_r p(s', r | s, a) [r + gamma*v_k(s')]\n",
    "\n",
    "    1. We start by initializing an array of the state-value function for each possible state. [v1(s1), v1(s2), ..., v1(sn)]. Each of these is initialized with a random value except for the v1(sn) = 0 as this is the expected reward at the terminal state.\n",
    "\n",
    "    2. We then perform 1 iteration of the IPE where we iterate over each state calculating the expected reward for that state using the immediate reward we get plus the expected reward for the state-value function on the next possible state.\n",
    "    *Keep in mind that this expected value on the next possible state is the expected reward that was randomly initialized in our array (v1(s'))*\n",
    "    So we now have the expected reward of our current state (summed over all possible actions, next states, and rewards) with our expected future reward added in (that was randomly initialized).\n",
    "\n",
    "    3. We keep repeating this iteration updating our state-value functions based on the reward we receive and the added in future expected rewards. Meaning the state-value at state s is updated (now at iteration k+1) using the state-value values of the successor states of s: s' (at iteration k) -> ( v_k+1 uses SUM_s' v_k(s') in its calculation).\n",
    "\n",
    "    4. Keep repeating this process until we reach some conversion point (as k approaches infinity in v_pi_k(s), v_pi_k(s) -> v_pi(s) Meaning we approach the proper expected return values of this specific policy). This conversion point is when the v_k(s) value is within a certain small threshold value difference from the previous iteration value of v_k-1(s) or in practice optionally some preset iteration count value.\n",
    "\n",
    "(For step 3.) Here we are using an (estimated/approximated) expected reward value of the next state instead of an averaged sampled reward estimation of the expected reward at a certain state. This is known as ***Expected updates***: \"they are based on an expectation over all possible next states rather than on a sample next state\". \n",
    "\n",
    "***Policy improvement theorem***: Way of challenging a polcicy to see if we can improve its performance. If we select a specific action a at state s and then follow the policy for the remaining steps and receive a better overall expected reward value than we would have if we followed the policy the entire time. It is thus better to select this specific a every time whenever in state s instead of whatever the policy would have chosen. Therefore we create that as our new and improved policy.\n",
    "\n",
    "    q_pi(s, pi'(s)) >= v_pi(s) --> v_pi'(s) >= v_pi(s)\n",
    "\n",
    "    Both policies are exactly the same except for that pi'(s) = a and pi(s) != a (some other action). (They only differ on this one state)\n",
    "\n",
    "We can create a general formula for improving the policy called ***policy improvement***. This is where we determine which action gives us the maximum possible expected reward using its current returned reward r, and the expected future reward of the next state using the old orginal policy.\n",
    "\n",
    "    pi'(s) = argmax_a q_pi(s, a)\n",
    "\n",
    "        = argmax_a E[R_t+1 + gamma*v_pi(S_t+1) | S_t = s, A_t = a]\n",
    "\n",
    "        = argmax_a SUM_s'_r p(s', r | s, a)[r + gamma*v_pi(s')]\n",
    "\n",
    "        return the index (the action we should take given s) of the action a that returns the highest expected reward (using the state-value function using the old policy)\n",
    "\n",
    "    v_pi'(s) = max_a E[R_t+1 + gamma*v_pi'(S_t+1) | S_t = s, A_t = a]\n",
    "\n",
    "        = max_a SUM_s'_r p(s', r | s, a)[r + gamma*v_pi'(s')]\n",
    "\n",
    "\\*Note this is the same formula for the optimal state-value function v_*(s). Therefore v_pi' must be equal to v_\\*, and both pi and pi' must be optimal policies. Since we now have a max added and equality to the optimal foruma we can conclude that the improvment theorem must provide an improved policy and the only way that it doesnt is if the original policy pi is already optimal (pi = pi_\\*).\n",
    "\n",
    "The reason that the original policy is improved to only an improved policy version and not improved to the optimal policy is because it uses the original policy in its future expected reward calculations. Thus it only improves the policy to select the action that gets the max reward now (R_t+1) and adds in the old future expected reward that is calculated from the non improved policy (as the policy is only now improved for state s and none of the other states (s').*\n",
    "\n",
    "***Policy iteration***: the policy improvement theorem improves a policy to an improved version. We can utilize this improvment theorem in multiple iterations to keep on improving our policy to get it closer and closer to an optimal policy. Steps:\n",
    "\n",
    "    1. The state-value function at our current policy pi_k is calculated for each state (this is done using iterative policy eval)\n",
    "    2. For each state we calculate the argmax action (the action that returns the highest expected reward) for our current state and check if the action differs from the action the policy would have chosen\n",
    "    3. Update the policy to use the actions for each state that returned the largest expected reward. This turns into our new policy pi_k+1\n",
    "    4. Repeat steps 1-3 until our policy no longer changes (already has all the max actions) or we hit a certain iterations limit and settle for a not optimal but improved policy.\n",
    "\n",
    "***Value iteration***: Instead of performing many interations of policy evaluations to get a state-value function (v_k) as close to the actual state-value function (v_pi) as possible we perform only one iteration of evaluation meaning we update the state-value function for each state only once. Then we perform the policy improvment. In value iteration we can combine both the 1 step policy evaluation and policy update into one formula:\n",
    "\n",
    "    v_k+1(s) = max_a E[R_t+1 + gamma*v_k(S_t) | S_t = s, A_t = a]\n",
    "            = max_a SUM_s'_r p(s', r | s, a)[r + gamma*v_k(s')]\n",
    "\n",
    "    v_j = v_k+i (at the iteration in which we decide that the state-value isnt changing that much anymore so we say its close enoguh to converged)\n",
    "\n",
    "    pi(s) = argmax_a SUM_s'_r p(s', r | s, a)[r + gamma*v_j(s')]\n",
    "\n",
    "    This way we set the newly created and improved policy to select the action that returns the max all-time expected reward. \n",
    "    This is done using v_j(s') as it has the memory of the return we got from the max actions (max_a) that were selected from state s'. \n",
    "    As well as the action taken at this current timestep to get the next state we need (s') with the probabilities in account.\n",
    "\n",
    "\"Policy iteration consists of two simultaneous, interacting processes, one making the value function consistent with the current policy (policy evaluation), and the other making the policy greedy with respect to the current value function (policy improvement)\"\n",
    "\n",
    "***Generalized policy iteration (GPI)***: The process of letting policy-evaluation and policy-improvment interact only based on each others results and not being affected or caring about the specific inner workings or processes of each other. \"The policy is always being improved with respect to the value function and the value function is always being driven toward the value function for the policy\". If the results from each of these two processes no longer change each other it means we have reached an optimal policy and optimal value function.\n",
    "\n",
    "The goal of the policy-improvement is to make the policy choose the greedy action (action with the highest expected reward) with respect to the state-value function (v_k(s))\n",
    "\n",
    "The goal of the policy-evaluation is to make the state-value function follow the policy (meaning the rewards we receive from the actions the policy selects should be reflected in the state-value function values).\n",
    "\n",
    "Since the Bellman optimality equation is v_*(s) = max_a SUM_s',r p(s', r | s, a)[r+gamma*v_*(s')] we can see how it relates to the policy following the max action and the state-value being reflected as that. Thus can conclude that this iterative process follows the optimal equation and will eventually reach that optimality with enough iterations.\n",
    "\n",
    "\\*The policy improvement pulls away from the state-value function as it switches its choices to the max thus being different from the choices of the state-value function, then the policy evaluation brings its state-value values to reflect those max choices which causes the max policy choices to no longer be different from the current policy. This creates a constant chase cycle that eventually converges to the optimal policy (once the changes no longer occur).*\n",
    "\n",
    "***Bootstrapping***: Updating the estimate of a value based on other estimates. Example: Expected updates use the expected reward value of the nest state s' (in addition to the current reward we receive from taking an action a at state s) to update the estimate of the expected reward of a policy at a certain state s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe9a471-f8de-4f42-a42b-197d37e11542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
