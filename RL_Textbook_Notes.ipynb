{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10250398-4f88-4955-8b8e-3774ec81122a",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ededc308-d24a-4cbe-bf0e-85244f64c789",
   "metadata": {},
   "source": [
    "***k-armed Bandit Problem***: Analogy of having several attempts at a slot machine with k levers. Each of the levers has an expected or mean reward value we receive if we decide to pull it (select lever 5 as our action at timestep 3 for example). The action selected at timestep t is denoted as A_t, the reward from this action is denoted as R_t, our arbitary action select as a, and the expected reward formula is thus: q*(a) = E[R_t | A_t = a]\n",
    "\n",
    "***Exploration vs Exploitation***: This is a tradeoff in RL, exploration is trying out new levers (with a potentially lower expected reward) with the hopes that we receive a much higher reward. Exploitation is using our current knowledge of expected rewards and selecting the action with the highest expected reward. We use different models to switch between these two efforts when we have our agent work through our map trying to collect rewards.\n",
    "\n",
    "***Sample-average***: Sampling method for creating an expected value estimate on an action. This method is simply the average reward value of the action up to this point. Meaning we sum all the rewards we got whenever we took this specific action then divided it by the amount of times we took this specific action. \n",
    "\n",
    "***Greedy action selection***: A_t = argmax_a Q_t(a) (select the action a that will maximize the value that Q_t (expected reward) will return.\n",
    "\n",
    "***epsilon-greedy methods***: methods that have a preset variable value epsilon that represents a probability that our agent will select an action randomly (all actions with equal probability) instead of the action that has the highest expected reward. Ex: eps=0.5 for the case of two actions at timestep 1: 50% chance of selecting action with highest expected prob and 50% chance of selecting randomly between action 1 and 2 (thus 0.50 + 0.25 = 0.75 chance we select the greedy option (excercise 2.1)).\n",
    "\n",
    "\n",
    "Advantage of eps-greedy over greedy depends on the task. For example, if the rewards have higher variance then it takes more exploration to find the true best rewards. If there is no variance then no exploration is needed and we can just do greedy. Can also run into ***nonstationary tasks*** where the true reward value of an action changes over time.\n",
    "\n",
    "***deterministic case***: models based on predefined logic and rules, no randomness is involved.\n",
    "\n",
    "Memory efficient way to calculate the sample-average reward (without storing all of the previous reward values). The update formula is: Q_n+1 = Q_n + (1/n)[R_n - Q_n] ==> NewEst = OldEst + StepSize[Target - OldEst]\n",
    "\n",
    "\n",
    "In nonstationary tasks we want to add more weighting to the most recent rewards observed (as tyring to get closest representation of the expected reward for the action currently (assuming it has changed)). We can change our stepsize function from the sample average function alpha(a) = (1/n) to a constant value alpha: alpha(a) = alpha. The sample average stepsize is gauranteed to converge to the true mean expected value while the constant value isnt. This is good becuase we dont want convergence on an expected value that is constantly changing.\n",
    "\n",
    "\n",
    "***Optimistic initial values***: setting the initial sample reward of each action much higher to what you think it will be. This encourages the agent to explore more at the start which can help for stationary problems. (Doesnt for non stationary as eventually expected reward will change and the inital reward values wont matter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5050e8af-1302-4f82-9be7-31c39f37c722",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77b75a4-5cba-41b6-aaa3-4f1a7f20096b",
   "metadata": {},
   "source": [
    "***Markov decision process***: model only depends on the previous state. In terms of RL, the model depends only on the previous state and action taken to determine its new state and current reward:\n",
    "\n",
    "    p(s', r | s, a) = Pr(S_t = s', R_t = r | S_t-1 = s, A_t-1 = a).\n",
    "    Creating the sequence: S_0,A_0,R_1,S_1,A_1,R_2,S_2,A_2,R_3,...\n",
    "\n",
    "Each probability function for state and reward returns a probability from a probability distribution. For each possible state (s) and action (a) we have a distribution of the probability for each of the next possible new_state and reward pairs occuring, given the previous state and action taken:  \n",
    "\n",
    "    SUM:s'_in_S SUM:r_in_R p(s', r | s, a) = 1, For_All s in S, a in A(s)\n",
    "\n",
    "***Markov property***: the likelihood of changing to a specific state is dependently only on the current state (and the current time elapsed), and not at all on any of the previous states. \"The state must include information about all aspects of the past agent–environment interaction that make a difference for the future\".\n",
    "\n",
    "***State transition probabilities***: The probability of transitioning from the current state to some specific next state. This is done by summing all of the reward probabilities for moving from the current state to this specific next state. In other words, we dont care about which reward we're gonna get, only what the probability is that we move to this next state:\n",
    "    \n",
    "    p(s' | s, a) = SUM:r_in_R p(s', r | s, a)\n",
    "\n",
    "***Expected reward for state-action pairs***: The expected reward we get for taking a specific action (a) at our current state (s):\n",
    "\n",
    "    r(s, a) = E[R_t | S_t-1=s, A_t-1 = a] = SUM:r_in_R *r* SUM:s'_in_S p(s', r | s, a)\n",
    "\n",
    "(Sum of each of the rewards (r) multiplied by the sum of each of the possible states we can get to (all s') if r was our reward)\n",
    "\n",
    "\n",
    "***Expected reward for state–action–next-state***: The expected reward we can expect to get if we take action (a) at our current state (s) and arrive at the new state (s'):\n",
    "\n",
    "    r(s, a, s') = E[R_t | S_t-1=s, A_t-1=a, S_t=s'] = SUM:r_in_R *r* p(s', r | s, a) / p(s' | s, a)\n",
    "\n",
    "    (Sum of each of the rewards (r) multiplied by the probability of getting our reward (r) at the new state (s') divided by the probability of getting our new state (s') from our current state (s) and action taken (a)).\n",
    "\n",
    "(The probability of getting a specific reward (r) at our specific next state (s') can be low, but if we factor in the probability of getting to this new state in general that will make our probability of receiving that reward realistic. In other words, what are the chances of getting our reward (r) in a specific scenerio given that this specific scenerio actually happens).\n",
    "\n",
    "***Agent–environment boundary***: Anything that is outside of the agents control and \"cannot be changed arbitrarely by the agent\" is considered to be out of the agents control and part of the enviornment. Thus the boundary represents the limit of the agents absolute control. For example, a robot (which is our agent) has mechanical arms and sensory devices, these are considered part of its enviornment and not part of the agent as they are reactive to the current enviornment and actions taken so far.\n",
    "\n",
    "***Reward hypothesis***: The agents goal is to maximize the total amount of reward it receives. Therefore not maximize immediate reward but average reward in the long run. \"That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward)\".\n",
    "\n",
    "***Episodes***: \"when the agent–environment interaction breaks naturally into subsequences, ... such as plays of a game, trips through a maze, or any sort of repeated interaction\". \n",
    "\n",
    "Each episode ends in a special state called the ***terminal state***.\n",
    "\n",
    "\"the next episode begins independently of how the previous one ended. Thus the episodes can all be considered to end in the same terminal state, with di↵erent rewards for the di↵erent outcomes. Tasks with episodes of this kind are called ***episodic tasks***\"\n",
    "\n",
    "***Continuing tasks***: \"in many cases the agent–environment interaction does not break naturally into identifiable episodes, but goes on continually without limit\". Ex: Robot with a long life span.\n",
    "\n",
    "Issue with continuing tasks is that our final time step is T=infinity, thus it is very easy for our agent to get a average reward of infinity and perform suboptimally as eventually it will reach a reward of infinity no matter how many sub optimal actions it takes. To solve this we use discounting in our rewards calculation.\n",
    "\n",
    "***Discounting***: Adding a hyperparameter that is a function of our current timestep. This hyperparameter multiplies our current reward by some preset constant value taken to the power of our current timestep:\n",
    "\n",
    "    G_t = R_t+1 + \u0000\u0000gamma*R_t+2 + gamma^2*R_t+3 + ... = inf_SUM_k=0 gamma^k * R_t+k+1\n",
    "\n",
    "gamma is in the range [0, 1] and refered to as the discount rate. \"Gamma determins the present value of future rewards, a reward received\n",
    "k time steps in the future is worth only \u0000gamma^(k\u0000-1) times what it would be worth if it were received immediately\". The longer you take (the larger your timestep gets) the less your reward value will be (creates sense of urgency for the agent).\n",
    "\n",
    "if gamma = 0 then we use none of the future reward calculations in our discounted reward so the model only maximizes the immediate reward and becomes greedy, versus as gamma gets closer to 1 it adds more weight to the future reward terms and the model becomes more faresighted.\n",
    "\n",
    "    G_t = R_t+1 + gamma*R_t+2 + gamma^2*R_t+3 + gamma^3*R_t+4 + ...\n",
    "    = R_t+1 + gamma(R_t+2 + gamma*R_t+3 + ...)\n",
    "    = R_t+1 + gamma*G_t+1\n",
    "\n",
    "\\*In these continuous tasks we have estimates for future reward to be received given a specific action a. These future rewards are based on the actions we expect the model to take and expected reward to receive (based on those future actions). And these future actions are the ones that will be available to us only if we take this certain action a. Thus this is a forward \"future\" thinking model and the gamma value is our hyperparameter for how much trust it has into these expected future values.*\n",
    "\n",
    "\n",
    "***Policy***: An agents strategy of behaviour (which actions it chooses) at a given time. \"Formally, a policy is a mapping from states to probabilities of selecting each possible action\". Example: in maze problem, dumb agents policyt is to wander around, while smart agents policy is to plan path in head first then go straight to the end goal. \n",
    "\n",
    "if agent is following policy pi() at time t, then pi(a|s) is probability that A_t = a, if S_t = s. \n",
    "The pi() function gives one probability but represents given a state s, ouput a probability distribution of actions the agent will take.\n",
    "\n",
    "Excercise 3.11: If the current state is St, and actions are selected according to stochastic policy ⇡, then what is the expectation of Rt+1 in terms of pi() and the four-argument function p (3.2)? -> pi(a|s) * r(s, a) forAll actions a in Action set of state s. the probability of taking the action multiplied by the expected reward value of taking that action at the current state s. (unsure on this one) not using the four argument function (3.2): p(s', r | s, a). \n",
    "\n",
    "***Deterministic policy***: Each state maps to a specific action with certanty. Meaning when we end up in state s1 we know that the policy will take action a1 with 100% probability.\n",
    "\n",
    "***Stochastic policy***: There is a distribution among the possible set of actions that can be taken from a certain state. Meaning there are different probabilities that the policy selects to take different actions at a state s. (Each of the probabilities from all the possible actions sum to 1).\n",
    "\n",
    "***State-value function for policy pi()***: The expected reward (from a continuous enviornment) we can expect to get if we start at state s and follow our specific policy pi().\n",
    "\n",
    "    v_pi(s) = E_pi[G_t| S_t = s] = E_pi[inf_SUM_k=0 gamma^k * R_t+k+1 | S_t=s ], forAll s in S\n",
    "\n",
    "***Action-value function for policy pi()***: The expected reward (from a cont. env.) we can expect to get if we start at state s, take action a, then follow our specific policy pi() after (after action a is taken at state s follow policy - what is the expected reward?).\n",
    "\n",
    "    q_pi(s, a) = E_pi[G_t| S_t = s, A_t = a] = E_pi[inf_SUM_k=0 gamma^k * R_t+k+1 | S_t=s, A_t = a ]\n",
    "\n",
    "***Monte Carlo method***: Calculating an average value from many samples taken from a random variable converges to the real expected value as the number of samples converges to infinity.\n",
    "\n",
    "***Optimal policy***: The optimal policy is the policy that gives us the highest summed reward over the long run. More formally as a form of ranking, a policy pi_1 is said to be better than or equal to a policy pi_2, if the expected return value of pi_1 is greater than or equal to the expected return of pi_2 for each state. Or: pi_1 >= pi_2 iff v_pi_1(s) >= v_pi_2(s) forAll s in S. Due to the greater than or equal to, there may be more than one optimal policy to which they are denoted as pi_*. \n",
    "\n",
    "Each share the same: \n",
    "\n",
    "***optimal state-value function***: \n",
    "    \n",
    "    v_*(s) = max_pi v_pi(s) forAll s in S. \n",
    "\n",
    "***optimal action-value function***: \n",
    "\n",
    "    q_*(s, a) = max_pi q_pi(s, a) forAll s in S, a in A = E[R_t+1 + gamma\\*v_\\*(S_t=t+1) | S_t = a, A_t = a]\n",
    "\n",
    "***Bellman optimality equation***: The value of a state value function following an optimal policy is equal to the expected return for the best action we can take from that state s.\n",
    "\n",
    "    v_*(s) = max_a_in_A(s) q_pi_*(s, a)\n",
    "    = max_a E_pi_*[G_t | S_t = s, A_t = a]\n",
    "    = max_a SUM_s'_r p(s', r| s, a)[r + gamma*V_*(s')]\n",
    "    select the action (a) that will return the max value of the probability of getting each reward at each possible next state multiplied by the reward we receive (for action a) summed with the gamma multiplied optimal state value function on that next possible state instance.\n",
    "\n",
    "    \n",
    "\n",
    "Meaning, starting at a state s, if we are following an optimal policy we will select the best possible action (in other words the action that has the highest expected reward)\n",
    "\n",
    "The same Bellman optimality logic applies for the action-value function: \n",
    "\n",
    "    q_*(s, a) = E[R_t+1 + gamma*Max_a' q_*(S_t+1, a') | S_t = s, A_t = a]\n",
    "    = SUM_s'_r p(s', r | s, a)[r + gamma* max_a' q_*(s', a')]\n",
    "\n",
    "\n",
    "The beauty of having an optimal state-value function is that we can create an agent that selects the action in a greedy way by only calculating the reward for this current state on which action to take (only looks in the present and not in the future). But the model is not short sighted because the optimal reward values from the state-value function are calculated with respect to the expected future rewards so the model is actually looking into the future.\n",
    "\n",
    "Often times we dont have either 1. an entire knowledge of the enviornment and its states 2. enough memory and computation power to calculate the optimal expected value for each state to get an optimal function to follow.\n",
    "\n",
    "***Tabular case***: An enviornment whos state set is small enough so that we can create an entry for a expected sampled reward value for each state (or each state-action pair) as a table or array in a computers memory.\n",
    "\n",
    "\"In many cases of practical interest, however, there are far more states than could possibly be entries in a table. In these cases the functions must be approximated, using some sort of more compact parameterized function representation.\"\n",
    "\n",
    "There can exist states that have an extremely low probability of being seen or stumbled upon by an agent that taking a suboptimal action for these states likely wont affect the overall expected total reward. Ex: Taking suboptimal: (0.000002)*(2) vs (0.000002)*4. (The prob of this state occuring (after a certain action) multiplied by the reward we get from it. Even though we took reward 2 instead of 4, with the probability being so low, our gain in expected reward doesnt change very much if at all).\n",
    "\n",
    "\"The online nature of reinforcement learning makes it possible to approximate optimal policies in ways that put more effort into learning to make good decisions for frequently encountered states, at the expense of less effort for infrequently encountered states. This is one key property that distinguishes reinforcement learning from other approaches to approximately solving MDPs.\"\n",
    "\n",
    "\"In reinforcement learning we are very much concerned with cases in which optimal solutions cannot be found but must be approximated in some way.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b79154-a48d-444a-baa0-ad95041154ec",
   "metadata": {},
   "source": [
    "# Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccfd5ab-caca-462c-b20f-b4c65694510c",
   "metadata": {},
   "source": [
    "***Policy evaluation (prediction problem)***: The process we go through or the problem of trying to figure out the expected reward from a given state s following policy pi. In other words figuring out the value of the state-value function at state s: \n",
    "    \n",
    "    v_pi(s) = E_pi[G_t | S_t = s]\n",
    "\n",
    "***Iterative policy evaluation***: An iterative approach to policy evaluation. For example, we know the reward we will get if we step into our goal state. But we then need to figure out the expected reward value of reaching all the possible states before the goal state and use that to figure out the expected reward value of all the states that can be used to reach those previous before reward states. This results in a system of linear equations that can be solved (only if we know the entire enviornment (aka know all possible states)) but this method uses an approximative iterative alternative approach.\n",
    "\n",
    "    v_k+1(s) = E_pi[R_t + gamma*v_k(S_t+1) | S_t = s ] \n",
    "    = SUM_a pi(a|s) SUM_s' SUM_r p(s', r | s, a) [r + gamma*v_k(s')]\n",
    "\n",
    "    1. We start by initializing an array of the state-value function for each possible state. [v1(s1), v1(s2), ..., v1(sn)]. Each of these is initialized with a random value except for the v1(sn) = 0 as this is the expected reward at the terminal state.\n",
    "\n",
    "    2. We then perform 1 iteration of the IPE where we iterate over each state calculating the expected reward for that state using the immediate reward we get plus the expected reward for the state-value function on the next possible state.\n",
    "    *Keep in mind that this expected value on the next possible state is the expected reward that was randomly initialized in our array (v1(s'))*\n",
    "    So we now have the expected reward of our current state (summed over all possible actions, next states, and rewards) with our expected future reward added in (that was randomly initialized).\n",
    "\n",
    "    3. We keep repeating this iteration updating our state-value functions based on the reward we receive and the added in future expected rewards. Meaning the state-value at state s is updated (now at iteration k+1) using the state-value values of the successor states of s: s' (at iteration k) -> ( v_k+1 uses SUM_s' v_k(s') in its calculation).\n",
    "\n",
    "    4. Keep repeating this process until we reach some conversion point (as k approaches infinity in v_pi_k(s), v_pi_k(s) -> v_pi(s) Meaning we approach the proper expected return values of this specific policy). This conversion point is when the v_k(s) value is within a certain small threshold value difference from the previous iteration value of v_k-1(s) or in practice optionally some preset iteration count value.\n",
    "\n",
    "(For step 3.) Here we are using an (estimated/approximated) expected reward value of the next state instead of an averaged sampled reward estimation of the expected reward at a certain state. This is known as ***Expected updates***: \"they are based on an expectation over all possible next states rather than on a sample next state\". \n",
    "\n",
    "***Policy improvement theorem***: Way of challenging a polcicy to see if we can improve its performance. If we select a specific action a at state s and then follow the policy for the remaining steps and receive a better overall expected reward value than we would have if we followed the policy the entire time. It is thus better to select this specific a every time whenever in state s instead of whatever the policy would have chosen. Therefore we create that as our new and improved policy.\n",
    "\n",
    "    q_pi(s, pi'(s)) >= v_pi(s) --> v_pi'(s) >= v_pi(s)\n",
    "\n",
    "    Both policies are exactly the same except for that pi'(s) = a and pi(s) != a (some other action). (They only differ on this one state)\n",
    "\n",
    "We can create a general formula for improving the policy called ***policy improvement***. This is where we determine which action gives us the maximum possible expected reward using its current returned reward r, and the expected future reward of the next state using the old orginal policy.\n",
    "\n",
    "    pi'(s) = argmax_a q_pi(s, a)\n",
    "\n",
    "        = argmax_a E[R_t+1 + gamma*v_pi(S_t+1) | S_t = s, A_t = a]\n",
    "\n",
    "        = argmax_a SUM_s'_r p(s', r | s, a)[r + gamma*v_pi(s')]\n",
    "\n",
    "        return the index (the action we should take given s) of the action a that returns the highest expected reward (using the state-value function using the old policy)\n",
    "\n",
    "    v_pi'(s) = max_a E[R_t+1 + gamma*v_pi'(S_t+1) | S_t = s, A_t = a]\n",
    "\n",
    "        = max_a SUM_s'_r p(s', r | s, a)[r + gamma*v_pi'(s')]\n",
    "\n",
    "\\*Note this is the same formula for the optimal state-value function v_*(s). Therefore v_pi' must be equal to v_\\*, and both pi and pi' must be optimal policies. Since we now have a max added and equality to the optimal foruma we can conclude that the improvment theorem must provide an improved policy and the only way that it doesnt is if the original policy pi is already optimal (pi = pi_\\*).\n",
    "\n",
    "The reason that the original policy is improved to only an improved policy version and not improved to the optimal policy is because it uses the original policy in its future expected reward calculations. Thus it only improves the policy to select the action that gets the max reward now (R_t+1) and adds in the old future expected reward that is calculated from the non improved policy (as the policy is only now improved for state s and none of the other states (s').*\n",
    "\n",
    "***Policy iteration***: the policy improvement theorem improves a policy to an improved version. We can utilize this improvment theorem in multiple iterations to keep on improving our policy to get it closer and closer to an optimal policy. Steps:\n",
    "\n",
    "    1. The state-value function at our current policy pi_k is calculated for each state (this is done using iterative policy eval)\n",
    "    2. For each state we calculate the argmax action (the action that returns the highest expected reward) for our current state and check if the action differs from the action the policy would have chosen\n",
    "    3. Update the policy to use the actions for each state that returned the largest expected reward. This turns into our new policy pi_k+1\n",
    "    4. Repeat steps 1-3 until our policy no longer changes (already has all the max actions) or we hit a certain iterations limit and settle for a not optimal but improved policy.\n",
    "\n",
    "***Value iteration***: Instead of performing many interations of policy evaluations to get a state-value function (v_k) as close to the actual state-value function (v_pi) as possible we perform only one iteration of evaluation meaning we update the state-value function for each state only once. Then we perform the policy improvment. In value iteration we can combine both the 1 step policy evaluation and policy update into one formula:\n",
    "\n",
    "    v_k+1(s) = max_a E[R_t+1 + gamma*v_k(S_t) | S_t = s, A_t = a]\n",
    "            = max_a SUM_s'_r p(s', r | s, a)[r + gamma*v_k(s')]\n",
    "\n",
    "    v_j = v_k+i (at the iteration in which we decide that the state-value isnt changing that much anymore so we say its close enoguh to converged)\n",
    "\n",
    "    pi(s) = argmax_a SUM_s'_r p(s', r | s, a)[r + gamma*v_j(s')]\n",
    "\n",
    "    This way we set the newly created and improved policy to select the action that returns the max all-time expected reward. \n",
    "    This is done using v_j(s') as it has the memory of the return we got from the max actions (max_a) that were selected from state s'. \n",
    "    As well as the action taken at this current timestep to get the next state we need (s') with the probabilities in account.\n",
    "\n",
    "\"Policy iteration consists of two simultaneous, interacting processes, one making the value function consistent with the current policy (policy evaluation), and the other making the policy greedy with respect to the current value function (policy improvement)\"\n",
    "\n",
    "***Generalized policy iteration (GPI)***: The process of letting policy-evaluation and policy-improvment interact only based on each others results and not being affected or caring about the specific inner workings or processes of each other. \"The policy is always being improved with respect to the value function and the value function is always being driven toward the value function for the policy\". If the results from each of these two processes no longer change each other it means we have reached an optimal policy and optimal value function.\n",
    "\n",
    "The goal of the policy-improvement is to make the policy choose the greedy action (action with the highest expected reward) with respect to the state-value function (v_k(s))\n",
    "\n",
    "The goal of the policy-evaluation is to make the state-value function follow the policy (meaning the rewards we receive from the actions the policy selects should be reflected in the state-value function values).\n",
    "\n",
    "Since the Bellman optimality equation is v_*(s) = max_a SUM_s',r p(s', r | s, a)[r+gamma*v_*(s')] we can see how it relates to the policy following the max action and the state-value being reflected as that. Thus can conclude that this iterative process follows the optimal equation and will eventually reach that optimality with enough iterations.\n",
    "\n",
    "\\*The policy improvement pulls away from the state-value function as it switches its choices to the max thus being different from the choices of the state-value function, then the policy evaluation brings its state-value values to reflect those max choices which causes the max policy choices to no longer be different from the current policy. This creates a constant chase cycle that eventually converges to the optimal policy (once the changes no longer occur).*\n",
    "\n",
    "***Bootstrapping***: Updating the estimate of a value based on other estimates. Example: Expected updates use the expected reward value of the nest state s' (in addition to the current reward we receive from taking an action a at state s) to update the estimate of the expected reward of a policy at a certain state s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcd2ad4-a8f7-47ad-bcde-b7b98f18742b",
   "metadata": {},
   "source": [
    "# Chapter 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0824f12d-369b-4f23-a96e-27232769d91d",
   "metadata": {},
   "source": [
    "***Monte carlo simulation***: Using repeated sampling of a random variable we can calculate the average and use that average as an estimate for the expected value for that random variable. Ex: randomly measure the height of many people and calculate the average as an estimate of the average height of a human.\n",
    "\n",
    "***First-visit monte carlo method***: Calculates a sample average of rewards for each state by iterating over multiple episodes and saving the return it gets from the first visit to that state. Therefore each sample of a reward for each state is the first reward we get when we visit a state in an episode. As the amount of iterations over episodes goes to infinity the sample average approaches the true expected value for the state reward. \"This is easy to see for the case of first-visit MC. In this case each return is an independent, identically distributed estimate of v_pi(s) with finite variance. ... Each average is itself an unbiased estimate, and the standard deviation of its error falls as 1/sqrt(n), where n is the number of returns averaged.\"\n",
    "\n",
    "***Exploring starts assumption***: Starting our visit method with a state-action value where each action has a probable chance of being selected. This gaurantees that we will have a reward estimate for each action at each state. This is done because when using monte carlo simulation to estimate the reward for a state-value function at state s, we use a policy to select an action. Thus the policy will select whichever action it thinks is optimal each time and some actions will never get explored.\n",
    "\n",
    "    For each episode:\n",
    "        Select first state s0, then (from a distribution where no options are zero) select an action a0 from s0: a0 in A(s0).\n",
    "        Then follow the policy after taking (s0, a0) -> s0, a0, r1, s1, a1, r2, .... st, at, rt+1 \n",
    "        (note that the policy takes control of selecting an action after (s0, a0), meaning in multiple episode iters we will have (s0, a0_k) -> s1, a1, r2, .... st, at, rt+1. Where a0 can be different each time but the following actions will be the same each time as the policy doesnt change/update).\n",
    "\n",
    "***Monte Carlo ES (Exploring starts) algorithm***: Algorithm used to find an optimal policy using the monte carlo method for calculating the value function for each state (the state-action value function q_pi_k(s, a))\n",
    "\n",
    "    1. init pi(s), Q(s, a), returns(s, a) for all states and actions\n",
    "    2.\n",
    "    Inf loop (for each episode)\n",
    "        select a random starting state si, then select an action from the action dist for si: ai\n",
    "        generate an episode starting from (s_i, a_i, r_i+1) -> (s_i, a_i, r_i+1), policy: (s_i+1, r_i+1), (s_i+2, r_i+2), ..., (s_T-1, r_T-1)\n",
    "        G = 0\n",
    "        for each step in episode (t=0 to T-1):\n",
    "            G = r_t+1 + gamma*G\n",
    "            (if (s_t, at) pair not in visited set then proceed)\n",
    "            add (s_t, at) as sample to the global average calc of Q(s, a) (of this specicic s and a)\n",
    "            perform policy iteration to update the policy towards taking the best action from the globally calculated action-state value: \n",
    "            pi(st) = argmax_a' Q(s_t, a')\n",
    "\n",
    "***On-policy learning***: We are actively trying to improve the policy that we are using to evaluate our state values. Generally has a ***soft-policy*** meaning that each action in each state has some probability that is greater than zero: pi(a | s) > 0 forAll s in S, a in A(s) and these probabilities gradually converge towards a deterministic policy where only one action is selected at each state a.k.a. the optimal policy.\n",
    "\n",
    "***On-policy first-visit MC control (for \"epsilon-soft policies)***: Follows the same algorithm as MC ES, however the difference is that instead of converging to a deterministic policy by setting the probability of non optimal actions to zero, it sets the probability of non optimal actions to the minimum probability (set by epsilon/|A(s)|) to ensure some exploration occurs. \n",
    "(epsilon/|A(s)| = minimum total prob for sub-optimal actions (epsilon) divided by the amount of actions possible for state s. Therefore the total prob that a suboptimal action gets selected is epsilon).\n",
    "\n",
    "    a* = argmax_a' Q(s_t, a')\n",
    "    pi(a | st) = { 1-eps + eps/|A(st)| -> if a = a*, else (a != a*): eps/|A(st)| }\n",
    "\n",
    "***Off-policy learning***: Instead of using the policy we are actively improving (and forcing to only select the optimal actions) to get samples and calculate the state-action values for each state-action pair (including the non-optimal actions). We create two policies instead, one for learning the optimal actions and learning the optimal policy called the ***target policy***, and one for exploring currently non optimal actions with the goal of exploring possibly better actions and getting accurate state-action values for each state-action pair called the ***behaviour policy***. \"In this case we say that learning is from data “off” the target policy\".\n",
    "\n",
    "***Assumption of coverage***: When comparing two policies if we assume that policy Z covers policy X we assume that for every action that policy X takes at each state s, if policy Z takes a different action at state s we assume that policy Z has at least a non zero probability for the action that policy X took. X(a | s) > 0 implies that Z(a | s) > 0 forAll s in S and a in A(s). (Policy X may be deterministic (meaning only takes one action with 100% chance at each state) but policy Z is stochastic in the states where its action selection is different). Thus in relating to off-policy learning, the target policy is often the deterministic greedy policy that must be covered by our behaviour policy.\n",
    "\n",
    "***Importance sampling***: technique used to estimate the expected values for one distribution given samples drawn from another distribution. This is done by using the ***importance-sampling ratio*** which is a measure of the relative probability of the samples occuring in the target set divided by the probability of the samples occuring in the observed set. For example:\n",
    "\n",
    "    observe the following from policy b: a_t, s_t+1, a_t+1, ..., s_T\n",
    "    The probability of it occuring under policy pi is: \n",
    "    Pr(a_t, s_t+1, a_t+1, ..., s_T | a_t, s_t+1, a_t+1, ..., s_T ~ b) \n",
    "    = pi(a_t | s_t)*p(s_t+1 | s_t, a_t) * pi(a_t+1 | s_t+1)*p(s_t+2 | s_t+1, a_t+1) * ... * pi(a_T-1 | s_T-1)*p(s_T | s_T-1, a_T-1)\n",
    "    = T-1_PROD_k=t pi(a_k | s_k)*p(s_k+1 | s_k, a_k)   \n",
    "    (probability of taking acton a_k at state s_k multiplied by probability of getting this next state s_k+1 given we are at state s_k and take action a_k).\n",
    "\n",
    "    Now we take the probability of this sequence occuring under policy pi and divide it by the probability of the sequence occuring under policy b:\n",
    "\n",
    "    greek_p_t:T-1 = T-1_PROD_k=t pi(a_k | s_k)*p(s_k+1 | s_k, a_k) / T-1_PROD_k=t b(a_k | s_k)*p(s_k+1 | s_k, a_k)   \n",
    "    (since p(s_k+1 | s_k, a_k) is the same for both we can cancel them out)\n",
    "    = T-1_PROD_k=t [ pi(a_k | s_k)/b(a_k | s_k) ]\n",
    "    (therefore is the total prob that we take each of the actions observed with policy pi divided by the prob that we took each action following policy b) This is essentially a measure on how accurate or close the observed actions from policy b are to policy pi\n",
    "\n",
    "    E[G_t | S_t = s] = v_b(s)\n",
    "    E[ greek_p_t:T-1 * G_t | S_t = s] = v_pi(s)\n",
    "    \n",
    "    let B = b(a_k | s_k), and PI = pi(a_k | s_k)\n",
    "    We have the ratio PI/B to adjust the samples from B to simulate them as samples for PI.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "    PI == B\n",
    "    Both samples have similar prob of occuring thus PI/B ~= 1 (meaning no change to influence of sample (its already accuratly represented) \n",
    "\n",
    "    PI low occurance\n",
    "    B high occurnace\n",
    "    if PI < B then PI/B < 1 (reducing its impact on PI calc)\n",
    "\n",
    "    PI high occurance \n",
    "    B low occurnace \n",
    "    if PI > B  then PI/B > 1 meaning we add extra influence to the occurance since its more common in PI than in B.\n",
    "\n",
    "***Ordinary importance sampling***: the value-state function in terms of the average samples from another policy:\n",
    "\n",
    "    Let F(s) = set of all timesteps in which state s is visited. (in first-visit method this will be only one visit per episode)\n",
    "    Let T(t) = the timestep that we terminate at from timestep t. (T(t) is the termination timestep of whichever episode timestep t belongs to).\n",
    "    {G_t}_t_in_F(s) = The returns for a state s at timestep t that accounts for all episodes (t in F(s))\n",
    "    {gp_t:T(t)-1}_t_in_F(s) = the importance sample ratio for the state s in the episode belonging to t\n",
    "\n",
    "    V(s) = ( SUM_t_in_F(s) gp_t:T(t)-1 * G_t ) / | F(s) |\n",
    "\n",
    "    \n",
    "***Weighted importance sampling***: The same process as the ordinary importance sampling except that we use a weighted average instead. The reason for the weighting is to reduce some of the importance sampling ratio's conversion onto our reward observation. For example, say that we get a ratio of *10 for our first observation (the observation is 10x more likely in pi() than in b()). This will result in a very biased initial value of *10 the observed expected value. We know that as we keep adding samples our overall average sample will converge down to normal but for the first few iterations this is too high. Adding the weighting reduces these initial additions and with enough iterations the weighting converges to zero so has no effect in the long term.\n",
    "\n",
    "    V(s) = ( SUM_t_in_F(s) gp_t:T(t)-1 * G_t ) / | SUM_t_in_F(s) gp_t:T(t)-1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81526f18-f126-4eaf-b90f-70da19c251f7",
   "metadata": {},
   "source": [
    "# Chapter 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55bfb13-4b1c-4f34-bb1b-23f150038a4c",
   "metadata": {},
   "source": [
    "***Temporal Difference***: Model free (not needing p(s', r | s, a) knowledge) learning method which learns by sampling the observed rewards like in Monte carlo and bootstrapping the reward estimate like is done in the dynamic programming approach.\n",
    "\n",
    "For policy evaluation (updating estimate of v_pi) using a simple every-visit monte carlo method (for nonstantionary problem) the update formula known as ***constant-alpha MC*** is: \n",
    "\n",
    "    v(S_t) = v(S_t) + alpha[G_t - v(S_t)]\n",
    "\n",
    "    G_t represents the actual return following from timestep t. \n",
    "\n",
    "Since we use G_t we have to wait until the end of the episode to update our value function as we need to step through to termination of the enviornment to get our actual return G_t. This is why the method is known as every-visit as we visit every state in our calculation.\n",
    "\n",
    "***TD(0) one-step TD***: Policy evaluation step similar to constant-alpha MC but instead of updating at the end of each episode we update at each timestep (this can be a huge benefit if we have very long episodes that we cant wait for to update our model). This is done by Replacing G_t with the current reward R_t+1 and the value estimate of the next state:\n",
    "\n",
    "    v(S_t) = v(S_t) + alpha[R_t+1 + gamma*v(S_t+1) - v(S_t)]\n",
    "\n",
    "***Sample Updates***: Update method utilized by the ones above. They update the value function based on the sampled immediate reward plus the sampled estimate value for the succeding (or next) state. The succeding next states value is also estimated as its immediate reward samples plus the sampled estimate of its next state. Almost exactly like ***expected updates*** except for the fact that we dont have a model (p(s', r | s, a)). This means that in sample updates the estimated sample reward value is based on only the next state observed and not on the estimates of every possible next state like it is in expected updates.\n",
    "\n",
    "    sample updates: \n",
    "    v(S_t) = v(S_t) + alpha[R_t+1 + gamma*v(S_t+1) - v(S_t)]\n",
    "    \n",
    "    expected updates:\n",
    "    v_k+1(s) = max_a SUM_s'_r p(s', r | s, a)[r + gamma*v_k(s')]\n",
    "\n",
    "    Notice how in sample updates we just add in S_t+1 to represent the next state we observe from our policy acting at state S_t.\n",
    "    While in expected updates we iterate over all the possible next states and rewards (SUM_s'_r) and sum the sampled rewards for each of those as the sampled future expected reward.\n",
    "\n",
    "A Temporal update is better than a monte carlo update in the case of a moving sample estimate for example. As we are updating the state-value immediately after an error occurs, it helps us in the case where we need to adjust to scenerios on the fly.\n",
    "\n",
    "***Batch updating***: A method for reusing your witnessed reward samples many times over in combination to update your state-value function until it gets close to convergence. This makes extra use of your samples and is especially useful when you have a expensive or limited enviornment and can only run so many episodes and timesteps. Example using TD learning:\n",
    "\n",
    "1. v(S_t) = v(S_t) + alpha[R_t+1 + gamma*v(S_t+1) - v(S_t)] is computed at each timestep t and instead of updating v(S_t) in place we store the resulting value\n",
    "2. At the end of some amount of episodes we sum up the values for each of the witnesses of v(S_t) for each state witnessed\n",
    "3. Then we increment the value of v(S_t) by its summed increment value repeatedly until the value converges or we reach some small change in v(S_t) (that lets us know we are very close to convergence)\n",
    "\n",
    "***Maximum-likelihood estimation***: The estimation on a parameter given some data that has the highest probability of giving us the data. For example if we measure the weight of 3 mice and find the values 3, 4, 5 lbs then the maximum likelihood estimation of the mean on mice weight distribution would be (3 + 4 + 5 / 3) = 4 as its more likely that we get those 3 weight readings from a distribution with the mean of 4 than any other mean value.\n",
    "\n",
    "Batch Monte Carlo methods returns a reward estimate that minimizes the mean-squared error on the training set whereas TD(0) finds the reward value that would be the maximum-likelihood for the Markov process.\n",
    "\n",
    "Batch TD(0) creates a state-value function that is an estimate of the true state-value function. It does this by taking many samples of rewards witnessed and calculating the maximum-likelihood model of the markov process (p(s', r| s, a)). In other words, based on the next states s' and rewards r witnessed TD(0) estimates p(s', r| s, a) inside v(s).\n",
    "\n",
    "***Certainty-equivalence estimate***: Using our estimated values of transitions and rewards and assuming they are accurate representations of our enviornment which then allows us to create a state-value function that is deemed accurate.\n",
    "Ex:\n",
    "\n",
    "    1. Witness transitions from state a as the following A -> B, A -> C, A -> B. Meaning from samples, our transition estimate is P(B | A) = 2/3, P(C | A ) = 1/3\n",
    "    2. Witness rewards A-> B: 1, A-> C: 0, A-> B: 1. Reward estimates E[r|A] = (1+1+0)/3\n",
    "\n",
    "    We assume these are the true expected reward and transition probabilities as we need the true values to calculate a state-value function.\n",
    "\n",
    "\\*As we take more and more samples, our estimates get closer and closer to the true values. Thus the certainty-equivalence estimate is the assumption that as we take more and more samples, we get to the true values which are then used to calculate the true state-value function.*\n",
    "\n",
    "***Sarsa***: An on-policy TD control algorithm (obtaines a value for a state-action value). We are using our current policy to create estimate values for state-action pairs. Described as sarsa because we use a (state, action, reward, state, action) pairing in our calculation. \n",
    "\n",
    "Sarsa (on-policy TD control) for estimating Q ~= q*\n",
    "1. Init all Q(s, a) to random values for all non terminal states and all actions, (Q(terminal_state, .) = 0)\n",
    "2. Loop through each episode starting with a random state S, then choose an action a from the state based on the current state-action values (epsilon-greedy action selection)\n",
    "3. Loop through the remaining states grabbing the next state s' from state s and action a and choosing an action for the next state\n",
    "\n",
    "       Init Q(S, A) for all s in S+, a in A(s), (Q(terminal, .) = 0)\n",
    "       Init S\n",
    "       Choose A from S using policy derived from Q (e.g. epsilon-greedy)\n",
    "       For Each Step of episode\n",
    "   \n",
    "           Take action A, observe R, S'\n",
    "           Choose A' from S' using policy derived from Q (e.g. epsilon-greedy)\n",
    "           Q(S, A) = Q(S, A) + alpha*[R + gamma*Q(S', A') - Q(S, A)]\n",
    "           S = S'\n",
    "           A = A'\n",
    "\n",
    "***Prediction Problem vs Control Problem***: \n",
    "    A prediction problem is to evaluate the quality of a given policy, in other words estimating the value function for a given policy. (Predicting the expected future rewards of a policy).\n",
    "    Control problem us to find the best policy that maximizes our expected reward. (Controlling the agent to get the best possible actions).\n",
    "\n",
    "A control algorithm's goal is to find the best policy that an agent can follow to get the maximal expected return over the long run.\n",
    "\n",
    "***Q-learning***: an off-policy TD control algorithm, similar to Sarsa in that it updates the state-action value Q, however it differs in that its an off-policy algorithm meaning that it updates its values based off of modifying the current policy to be greedy on the next state-action pair. This means that it directly approximates the optimal state-action values q*(s, a) (as those are the ones that use the greedy actions). Sarsa updates its state-action pair based on the immediate reward and the expected reward of the state-action pair that its current policy selects, meaning it learns on (or using) its current policy. \n",
    "\n",
    "    Q(S, A) = Q(S, A) + alpha*[R + gamma*max_a'Q(S', a') - Q(S, A)]\n",
    "    Because we are taking the max action for the future expected reward this allows us to learn the optimal policy directly regardless of the actions our current policy wants to take.\n",
    "\n",
    "Sarsa vs Q-learning:\n",
    "\n",
    "- Sarsa learns on-policy meaning it follows the actions its current policy takes\n",
    "- Q-learning learns off-policy meaning it doesnt follow its current policy and instead acts in a greedy manner\n",
    "- Q-learning is more aggressive in its learning since it always follows the best returning action, while Sarsa is more conservative in that its not swayed by the current values of its state-action pairs.\n",
    "\n",
    "***Expected Sarsa***: Extension of sarsa and similar to Q-learning in that instead of taking the max action for the next state expected reward we take the overall expected reward meaning it includes all of the possible actions:\n",
    "\n",
    "    Q(S_t, A_t) = Q(S_t, A_t) + alpha*[R + gamma*E_pi[Q(S_t+1, A_t+1) | S+t+1] - Q(S_t, A_t)]\n",
    "                = Q(S_t, A_t) + alpha*[R + gamma*SUM_a pi(a | S_t+1)*Q(S_t+1, a) - Q(S_t, A_t)]\n",
    "\n",
    "    \"this algorithm moves deterministically in the same direction as Sarsa moves in expectation\"\n",
    "    Generally performs better than Sarsa as it removes the variance of randomly selecting actions (and taking the chance that they arent optimal actions).\n",
    "\n",
    "Expected Sarsa can be off-policy and on-policy. (If current policy is greedy while the behaviour is more exploratory then expected sarsa is exactly Q-learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac57bd4-7f75-432b-a670-9987a1426333",
   "metadata": {},
   "source": [
    "# Chapter 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f891e96b-769d-4f0c-8b5c-27d7330d6395",
   "metadata": {},
   "source": [
    "**Model of enviornment**: Anything agent can use to predict how the enviornment will respond to its actions.\n",
    "\n",
    "**Distribution models**: When a model produces a list of all possible actions/posibilities (in an enviornment setting) along with their probabilities. Ex: rolling two dice, distr model would produce each possible combination of both dice values along with the probability of it occuring.\n",
    "\n",
    "**Sample models**: When a model produces one possible action/posibility (in an env setting) sampled from the probability distribution. Ex: rolling two dice, sample model would produce a single pair of values sampled from the distr of possible dice pair values.\n",
    "\n",
    "**Planning**: Any computational process that takes a model as input and returns a improved policy (that is meant for interacting with the enviornment represented in the model). \n",
    "\n",
    "    planning(model) -> policy.\n",
    "\n",
    "\n",
    "Planning vs Learning: \"From an MDP standpoint, planning is about coming up with the policy that maximizes expected long-term rewards. Learning is about estimating the transition probabilities, and sometimes the reward distribution for each transition.\"\n",
    "\n",
    "**Model-learning**: Using the rewards and states observed (known as real experience) by a planning agent to improve our model of the enviornment. Meaning we use our samples to improve p(s', r | s, a). \n",
    "\n",
    "**Direct reinforcement learning**: Using the rewards and states observed to improve our state-value functions and policy. Known as direct due to using real experience to directly improve our value functions and policy.\n",
    "\n",
    "model learning is known as an indirect RL method as by improving our model we indirectly improve our value functions and policy due to both of them using the model in their calculations.\n",
    "\n",
    "**Search control**: Selecting starting states and actions as simulated experiences for training a model. For example, for Random-sample one-step tabular Q-planning has the following steps:\n",
    "\n",
    "    1. Select a state, s in S, and an action, a in A(s), at random \n",
    "    2. Send s, a to a sample model, and obtain a sample next reward, r, and a sample next state, s' \n",
    "    3. Apply one-step tabular Q-learning to S, A, R, S':\n",
    "        \u0000Q(s,a) = \u0000Q(s,a) + step_size[R+\u0000 max_a Q(s',a) - \u0000Q(s,a)]\n",
    "\n",
    "    In this algorithm, step 1 is search control. We are randomly selecting starting points to simulate an experience in our enviornment in which we can learn from.\n",
    "\n",
    "**Dyna-Q**: Algorithm for learning an optimal policy that classifies as a online planning method. It combines model free learning (in calculating the state-value function) with model based methods (to grab samples from our enviornment).\n",
    "\n",
    "Dyna-Q vs regular Q-learning: Both algorithms start out with an intial step of starting at a state in the enviornment and taking a real life step to observe a reward. The difference is that Q-learning will stop after this step do its value calculations and then repeat, while Dyna-Q stores this real life result in a table and then performs simulation steps based on the table info to do further calculations. Example of real life for Dyna-Q:\n",
    "\n",
    "\n",
    "\n",
    "    Agent in state S1.\n",
    "    \n",
    "    Takes action A1.\n",
    "    \n",
    "    Receives reward R1 and transitions to state S2.\n",
    "    \n",
    "    Update: Q(S1, A1) = Q(S1, A1) + step_size[R1 + gamma*max_a' Q(S2, a') - Q(S1, A1)]\n",
    "    \n",
    "    Model Update: (Learn from real experience) Model(S1, A1) = (S2, R1) \n",
    "    \n",
    "    Simulated Experience:\n",
    "    \n",
    "        Using the model:(S2, R1) = Model(S1, A1)\n",
    "    \n",
    "        Simulate and update Q-values: Q(S1, A1) = Q(S1, A1) + step_size[R1 + gamma*max_a' Q(S2, a') - Q(S1, A1)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Dyna-Q+**: Dyna-Q with an added heuristic that helps the agent explore states that havent been explored in a while. The heuristic adds more reward to the states based on for how many timesteps they havent been explored. The reward calculation is as follows: r + k*sqrt(t) where t is the number of timesteps that a state hasnt been visited. This method helps an agent explore new paths that could be better solutions than the one it currently has. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a06130-1d48-437d-9649-d270302cf6f1",
   "metadata": {},
   "source": [
    "# Chapter 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace08c91-314a-4c45-b582-cac5b8fce4b8",
   "metadata": {},
   "source": [
    "***Supervised Learning methods***: In terms of RL, we utilize ***function approximation*** to aproximate our state-value function using some kind of learning model like a neural network or a decision tree for example. We define our approximation function as v-hat(s, w) ~= v_pi(s) where s is the state and w is a vector of weights. We then treat each action as a sample in our model and shift our state-value towards the target (which is our experienced reward and estimated future reward from the approximation function). \n",
    "\n",
    "    TD(0): S_t -> R_t+1 + step_size*v-hat(s, w)\n",
    "    TD(N): S_t -> G_t:t+n\n",
    "\n",
    "    These above are known as our training samples.\n",
    "    Our target is the actual G value of the state which we use as the target. \n",
    "    In this case TD(0) is like the prediction and TD(N) is the target and we use that to update w in v-hat(s, w)\n",
    "\n",
    "In RL we must select our models based on our enviornment. Often we need one that can run in an online setting and can learn to a moveint target ie. nonstationary target.\n",
    "\n",
    "***Mean Squared Value Error***: The error/objective function used to evaluate our state-value function approximator. Since we often have fewer weights than states, when we update the weights to account for a sample witnessed at state s_i to improve v-hat(s_i, w) it can decrease the accuracy of v-hat(s_j, w) (decreased the acc at other states). Therefore, we add a weighting towards each state that lets us know which states we want to be an accurate as possible in and which states we can afford to be somewhat wrong in. This weighting value is mu(s) >= 0, SUM_si mu(s_i) = 1.\n",
    "\n",
    "    VE-bar(w) = SUM_s_in_S mu(s)[v_pi(s) - v-hat(s, w)]^2\n",
    "\n",
    "    Since our goal is for v-hat(s, w) ~= v_pi(s), the fdifference between the two is our error (we square it) then multiple it by the weights importance weighting.\n",
    "\n",
    "***On-policy distribution***: the fraction of time spent in each state as a measure of its importance (for mu(s)). Formula:\n",
    "\n",
    "    h(s) - probability that an episode starts in state s\n",
    "    eta(s) - the average number of time steps spent in state s in an episode\n",
    "\n",
    "    eta(s) = h(s) + SUM_s-bar eta(s-bar) * SUM_a pi(a | s-bar) * p(s | s-bar, a) for all s in S\n",
    "\n",
    "    Time is spent in state s if an episode starts in state s (h(s)) OR (+) if a transition is made from a preceeding state (s-bar) into state s.\n",
    "    The preceeding transition into state s is: over all actions, prob of taking action given prev state multiplied by prob of getting state s given we took action a from the prev state s-bar.\n",
    "\n",
    "    mu(s) = eta(s) / SUM_s' eta(s') for all s in S\n",
    "\n",
    "    The amount of timesteps spent in a state normalized by the total amount of timesteps spent in each state.\n",
    "    If discounting exists (gamma < 1) then we include it in the SUM_s-bar portion of the eta(s) = formula.\n",
    "\n",
    "***Global optimum***: Weight vector that has the least amount of error.\n",
    "\n",
    "    VE-bar(w*) <= VE-bar(w) for all possible w\n",
    "\n",
    "***Local optimum***: Weight vector that has the leat amount of error in some neighborhood of w*.\n",
    "\n",
    "Often in non-linear approximation functions we have to settle for local optimums instead of global ones.\n",
    "\n",
    "Stochastic gradient descent in terms of updating the weights for the state-value approximation:\n",
    "\n",
    "    w_t=1 = w_t - (1/2)*step-size*delta_grad[v_pi(s_t) - v-hat(s_t, w)]^2\n",
    "          = w_t - step-size*[v_pi(s_t) - v-hat(s_t, w)] * delta_grad(v-hat(s_t, w))\n",
    "\n",
    "    delta_grad(f(w)) = [deriv(f(w))/deriv(w1), deriv(f(w))/deriv(w2), ..., deriv(f(w))/deriv(wd)]^T\n",
    "\n",
    "For SGD we can use monte carlo to create an unbiased estimate. Let U_t be our target, with monte carlo: U_t = G_t then we have:\n",
    "\n",
    "    w_t+1 = w_t + step-size[U_t - v-hat(s_t, w)]*delta_gradv-hat(s_t, w)\n",
    "          = w_t + step-size[G_t - v-hat(s_t, w)]*delta_gradv-hat(s_t, w)\n",
    "\n",
    "    If U_t is an unbiased estimate then: E[U_t| S_t = s] = v_pi(S_t)\n",
    "\n",
    "***Semi-gradient methods***: Instead of using the true state-value function as our target we use our function approximation that uses the weights vector w. Since both the target and prediction use our weights w, it is not a true value and instead an estimate of it. Therefore, we call it a semi-gradient since the difference between the target and observation is not the true gradient that will move us down the steepest path.\n",
    "\n",
    "    Ex: U_t = SUM_a_s'_r pi(a | s) p(s', r | s, a)[r + gamma*v-hat(s', w_t)\n",
    "    or\n",
    "    TD(0) semi-gradient target: U_t = R_t+1 + gamma*v-hat(S_t+1, w)\n",
    "\n",
    "Semi-gradient allows for faster learning, although they dont converge in all cases like full-gradient methods they do for linear cases which is useful. As well as allowing for the ability to be implemented in continual and online learning (before an episode ends).\n",
    "\n",
    "***State aggregation***: Generalizing function approximation method that groups similar states into groups which we then use as state-value functions. For example, if we have 1000 states and 100's of the states all have very similar values then we can simply and move each into a group to create 10 groups and estimate the value of each group instead.\n",
    "\n",
    "***Feature vector***: Representing our state with a vector where each value represents a numeric feature of the state.\n",
    "\n",
    "    x(s) = (x1(s), x2(s), ..., xd(s))^T   (1 to d means we get one feature for each weight in our weight vector w)\n",
    "\n",
    "    v-hat(s, w) = w^T * x(s) = d_SUM_t=1 w_i * x_i(s)\n",
    "\n",
    "    gradient_delta(v-hat(s, w)) = x(s)\n",
    "    ->\n",
    "    w_t+1 = w_t + step_size[U_t - v-hat(s_t, w)]*x(s)\n",
    "\n",
    "In the linear case there is no local optimal only global optima therefore we can use semi-gradient methods to get to or near the global optimal.\n",
    "\n",
    "***Polynomial feature vectors***: Creating polynomial interactive features so that we can represent specific data with certain prior beleifs, we still use linear weights to represent a linear relationship. Ex: each state has two peices of info s1 and s2, we know that when s1 is low and and s2 is high we have to take a specific action. To represent this relationship we have to create the feature vector like so:\n",
    "\n",
    "    x(s) = (1, s1, s2, s1*s2)^T\n",
    "    or\n",
    "    x(s) = (1, s1, s2, s1*s2, s1^2 * s2^2)^T\n",
    "\n",
    "***Fournier Basis***: Linear function approximation method that expreses periodic functions as sums of sins and cosine features. (A function f is periodic if f (x) = f (x + y) for all x and some period y).\n",
    "\n",
    "***Coarse Coding***: Visual method for representing a state in terms of its binary features inside a 2d plot. This plot contains circles that represent a binary feature, the circles may intersect with each other meaning that the features share some common data and that its possible for a state to have both features at the same time. The circles can also be shaded in or blank, shaded in represents a feature that exists for a plotted state and a blank circle means that this feature does not exist in this particular state. We can also plot multiple states and whichever circles are double shaded means that we can see a visual of how similar two states are (which features they share). We can adjust the size of our circles to have different effects during training. For example, smaller circles wont intersect with each other as much meaning a state will only use a small amount of features hence narrow generalization from the features, we can increase the circle size to get broad generalization or stretch them asymmetricly to get a asymmetric generalization. These adjustments come with tradeoffs for example, broad circles will have a broader generalization among weights but will lose out on some perciseness needed for discriminating between similar samples. Few specific features also limits the amount of generalization our weights can do to other samples. So there is a tradeoff. \n",
    "\n",
    "***Tile Coding***: A widely used version of coarse coding that applies to multi-dimensional continuous spaces. The difference is instead of having one 2d plot with binary features in it we create several different versions of our features called **partitions (or tillings)** and each partition is made up of its own binary features that are represented as **tiles** and take up the whole partition grid. Then we overlayer the partitions (offsetting each by some width value so that multiple tiles can overlap) and when we plot a state we can have the intersection of multiple binary features, one from each partition to add multidimensional feature data to a state. At each state plotting we have a gaurantee of a state activating exactly one tile from each parition which gives us mathematical consistency for selecting a constant step size that doesnt have to vary based on the amount of features activated for a state. (Lets us set a step size that consistently improves our weight by taking a constant value amount from each tile feature).\n",
    "\n",
    "Tile parameters: \n",
    "\n",
    "- Offset size:\n",
    "    We can select how to offset the tilings from each other and this affects our generalization pattern. As shifting the tillings from each other horizontally will cause the tiles catched by a state to be the horizontal features in each tilling. We can also select the proportion or length that each tilling is shifted a certain length a fraction value of tile width/number of tillings lets us know which features will be activates from each tilling based on a states position.\n",
    "\n",
    "- Shape of tiles:\n",
    "    Selecting the tile shape also affects how generalization occurs, square tiles ensures that each feature is represented equally, we can add log stripes that show more bias and inclusion of much more features on lines that occur more closely together on one side and more seperated on another side. As well as diagnol stripes that promote generalization among a diagnal dimension.\n",
    "\n",
    "***Hashing***: Memory efficient size reduction algorithm that reduces a tile size to randomly spread out smaller tiles that will still activate as the feature but only in the spaces that we really need. \n",
    "\n",
    "***Radial Basis Function***: Generalization method for converting coarse coding to a continuous version where instead of binary features we use a gaussian bell shaped function that allows for values anywhere between 0 and 1 that represent the degree to which a feature is present in a state rather than a exists or doesnt exist metric. Although this function is differentiable, it is a high cost function and mostly theory based and not applied in high dimensional cases due to the high cost. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89469cf6-4c9b-4d69-8e6e-776de016f158",
   "metadata": {},
   "source": [
    "# Chapter 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cfc2c0-375c-4448-9a94-b299e7fb6fda",
   "metadata": {},
   "source": [
    "***Episodic semi-gradient one-step Sarsa***: Semi gradient update for weight vector in estimating U_t or the state-action value.\n",
    "\n",
    "    w_t+1 = w_t + step_size*[U_t - \u0000q-hat(S_t,A_t,w_t)] * (gradient of) delta_\u0000q-hat(S_t,A_t,w_t)\n",
    "    = w_t + step_size*[R_t + gamma*q-hat(S_t+1,A_t+1,w_t) - \u0000q-hat(S_t,A_t,w_t)] * delta_\u0000q-hat(S_t,A_t,w_t)\n",
    "\n",
    "To create a control method from this (improve the policy) we can create a A_t* = argmax_a q-hat(S_t, a, w_t) using a soft approximation like epsilon-greedy to update the weights with an improved action. \n",
    "\n",
    "In terms of representing a problem with a feature vector, one method is to use tile coding to create x(s, a) then we can represent:\n",
    "\n",
    "    q-hat(s,a,w) = w^T * x(s, a) = d_SUM_i=1 w_i * x_i(s, a)\n",
    "\n",
    "***Semi-gradient n-step Sarsa***: extends the semi gradient definition above to use a longer series of actually observed rewards instead of only the next one.\n",
    "\n",
    "    G_t:t+n = R_t+1 + gamma*R_t+2 + ... + gamma^(n-1) * R_t+n + gamma^n * q-hat(S_t+n, A_t+n, w_t+n-1)  (t+n < T)\n",
    "    if t+n >= T then we assume that we have the entire episodic reward sequence thus an unbiased estimate:\n",
    "\n",
    "        w_t+n = w_t+n-1 + step_size*[G_t:t+n - \u0000q-hat(S_t,A_t,w_t+n-1)] * delta_\u0000q-hat(S_t,A_t,w_t+n-1)\n",
    "\n",
    "There is a tradeoff between n-step and one-step and often the best performance is when an intermediate level of \"bootstrapping\" is used and we have something like n=8 for example. Large n-step values have higher variance in our learning as we can take certain actions that arent optimal and receive a lot of suboptimal rewards that will affect our scores early on, whereas one-step dont get enough data.\n",
    "\n",
    "***Average-reward setting***: Reward measurment technique for continuous problems that doesnt use the discounting rule (since discounting is problematic with function approximation). The quality of a policy is defined as the average rate of reward obtained from following our policy in this continuing state. Denoted as r(pi):\n",
    "\n",
    "    r(pi) = lim_h->inf 1/h h_SUM_t=1 E[R_t | S_0, A_0:t-1 ~ pi]\n",
    "    = lim_t->inf E[R_t | S_0, A_0:t-1 ~ pi]\n",
    "    = SUM_s mu_pi(s) * SUM_a pi(a | s) * SUM_s',r p(s', r | s, a) * r\n",
    "\n",
    "    expected reward is continioned on the starting state S_0 and the sequence of actions that the policy takes A_0:t-1 ~ pi\n",
    "\n",
    "***Steady-state distribution (mu_pi(s))***: The probability of each state given our sequence of actions taken from the policy:\n",
    "\n",
    "    mu_pi(s) = lim_t->inf Pr{St = s |A_0:t\u0000-1 ~ pi}\n",
    "\n",
    "***Differential return***: Defining the total reward using the average return instead of with discounting:\n",
    "\n",
    "    G_t = R_t+1 - r(pi) + R_t+2 - r(pi) + R_t+3 - r(pi) + ...\n",
    "\n",
    "We also have differential bellman equations where we replace all rewards and gammas with: r - r(pi). Ex:\n",
    "\n",
    "    v_pi(s) = SUM_a pi(a | s) SUM_s'_r p(s', r | s, a)[r - r(pi) + v_pi(s')] (instead of [r + gamma*v_pi(s')])\n",
    "\n",
    "    v_*(s) = max_a SUM_s'_r p(s', r | s, a)[r - max_pi r(pi) + v_*(s')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
